__author__ = 'kolosnjaji'

from collections import Counter
from numpy import nanmean

import tensorflow as tf
from tensorflow.models.rnn import rnn_cell

import pickle
import numpy as np
import random
import sys

import copy

apicalls_file = open('/local_home/kolosnjaji/data/holmes_data/cuckoo_data/apicalls_all.txt', 'r')
all_sequences = apicalls_file.readlines()
sequences_split = []
sequences_split = [x.split() for x in all_sequences]

md5_file = open('/local_home/kolosnjaji/data/holmes_data/sha256list_virustotal.bin', 'rb')
md5_hashes = pickle.load(md5_file)

md5_dictionary_fs = pickle.load(open('/local_home/kolosnjaji/data/cluster_map_fs_holmes.bin','rb'))

my_counter = Counter(md5_dictionary_fs.values())

print my_counter


sequences_split_wf = []
md5_hashes_wf = []

for hash in md5_dictionary_fs.keys():
    sequence_file = open('/local_home/kolosnjaji/data/holmes_data/cuckoo_data/' + hash.lower() + ".txt", 'r')
    sequence_split = sequence_file.readlines()
    sequence_split_new = []
    for i,seq in enumerate(sequence_split):
        if (i<2):
            sequence_split_new.append(seq)
        elif((sequence_split[i-1]!=sequence_split[i]) or (sequence_split[i-1]!=sequence_split[i-2])):
            sequence_split_new.append(sequence_split[i])
        else:
            pass                               
    if (len(sequence_split_new)>100):
        sequence_split_new = sequence_split_new[0:100]
    for j in range(len(sequence_split_new)):
        sequence_split_new[j] = sequence_split_new[j][0:-1] # strip the line terminators

    if (md5_dictionary_fs[hash]>-1):
        if len(sequence_split_new)>0:
            sequences_split_wf.append(sequence_split_new)
            md5_hashes_wf.append(hash)
        else:
            pass


word_list = []
len_max = 0
for seq in sequences_split_wf:
    len_seq = len(seq)
    if (len_seq>len_max):
        len_max = len_seq
    for word in seq:
        if not word in word_list:
            word_list.append(word)

dict_size = len(word_list)

print "Word (unique system call) count: {0}".format(dict_size)

print "Maximal sequence length is: {0}".format(len_seq)

unity_matrix = np.eye(dict_size)

feature_matrices = np.zeros((len(sequences_split_wf), len_max, dict_size))
feature_mask = np.zeros((len(sequences_split_wf), len_max, dict_size))

# sort api calls to families

for i,seq in enumerate(sequences_split_wf):
    for j,word in enumerate(seq):
        ind = word_list.index(word)
        feature_matrices[i,j,:] = unity_matrix[ind,:]
        feature_mask[i,j,:]+=1

num_tests_nominal = 1

np.set_printoptions(precision=3, linewidth=150)

malware_dict_pattern = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}

test_cuckoo_accuracy = copy.deepcopy(malware_dict_pattern)

num_families = len(malware_dict_pattern.keys())

test_conf_matrix = np.zeros((num_families, num_families))

def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2,1, 1],
                        strides=[1, 2, 1, 1], padding='SAME')



precision_all_tests = []
recall_all_tests = []
accuracy_all_tests = []

[precision_all_tests.append([]) for i in range(0,num_families)]
[recall_all_tests.append([]) for i in range(0, num_families)]
[accuracy_all_tests.append([]) for i in range(0, num_families)]


cv_k=2

random.seed(1337)

for test in range(num_tests_nominal):

    accuracy_cv = []
    precision_cv = []
    recall_cv = []
   
    random_integers = range(len(sequences_split_wf))

    print "Total data: {0}".format(len(random_integers))
    

    random.shuffle(random_integers)

    num_indices = int(len(random_integers)/float(cv_k))

    print "Num indices {0}".format(num_indices)


    for cv_iter in range(cv_k):
        if (cv_iter+1)*num_indices > len(random_integers):
            max_num = len(random_integers)
        else:
            max_num = (cv_iter+1)*num_indices
        test_set_indices = random_integers[cv_iter*num_indices:max_num]
        training_set_indices = []
        for i in range(len(random_integers)):
            if not random_integers[i] in test_set_indices:
                training_set_indices.append(random_integers[i])
        training_set_length = len(training_set_indices)
        test_set_length = len(test_set_indices)

        print "Training set length:{0}".format(training_set_length)
        print "Test set length:{0}".format(test_set_length)

        malware_training_set = np.zeros((training_set_length, len_max, dict_size))
        malware_training_mask = np.zeros((training_set_length, len_max, dict_size))

        label_size = num_families

        malware_training_families = np.zeros((training_set_length, label_size))
        malware_test_families = np.zeros((test_set_length, label_size))


        malware_test_set = np.zeros((test_set_length, len_max, dict_size))
        malware_test_mask = np.zeros((test_set_length, len_max, dict_size))


        sequences_split_training = []
        md5_hashes_training = []

        for seq_index in range(0, training_set_length): # create training set of sequences (sequences_split_training, malware_dict_train[family])                       
            sequence = sequences_split_wf[training_set_indices[seq_index]]
            sequences_split_training.append(sequence)
            md5_hash_wf = md5_hashes_wf[training_set_indices[seq_index]]
            md5_hashes_training.append(md5_hash_wf)
            if (md5_hash_wf in md5_dictionary_fs):
                family = md5_dictionary_fs[md5_hash_wf]

                malware_training_families[seq_index, family] = 1


                malware_training_set[seq_index, :,:] = feature_matrices[training_set_indices[seq_index], :,:]
                malware_training_mask[seq_index,:,:] = feature_mask[training_set_indices[seq_index], :,:]

            else:
                print "No family found, weird."

        
        print "Number of training sequences: "  + str(len(sequences_split_training))


        
        sequences_split_test = []
        md5_hashes_test = []


        for seq_index in range(0, test_set_length): # creating test set of sequences (sequences_split_test, malware_familites_test[family])      
            sequence = sequences_split_wf[test_set_indices[seq_index]]
            sequences_split_test.append(sequence)
            md5_hash = md5_hashes_wf[test_set_indices[seq_index]]
            if (md5_hash in md5_dictionary_fs):
                md5_hashes_test.append(md5_hash)
                family = md5_dictionary_fs[md5_hash]
                malware_test_set[seq_index, :,:] = feature_matrices[test_set_indices[seq_index], :,:]
                malware_test_mask[seq_index,:,:] = feature_mask[test_set_indices[seq_index], :,:]
                malware_test_families[seq_index, family] = 1

        i=0
        print "Number of test sequences: "  + str(len(sequences_split_test))




######################################### training ######################################


        print np.any(malware_training_mask<0)
        print np.any(malware_training_families>1)
        print np.any(malware_training_families<0)

        print "=================================TRAINING======================================="

        # training static analysis (random forest, adaboost...)
        lstm_size = 50        
   
        # Initial state of the LSTM memory.
                       
        num_epochs = 150#150

        num_batches = 20

        batch_size = int(training_set_length/20)

        print "Batch size: {0}".format(batch_size)

        print "Max length: {0}".format(len_max)


        x = tf.placeholder("float", shape=[None, len_max, dict_size])
        y_ = tf.placeholder("float", shape=[None, label_size])
        mask = tf.placeholder("float", shape = [None, len_max, dict_size]) 
        keep_prob = tf.placeholder("float")

        W_conv1 = weight_variable([3,60,1,10]) # 3-grams, width of the filter
        b_conv1 = bias_variable([10])
    
        x_image = tf.reshape(x, [batch_size, len_max, dict_size,1])
    

        print "x_image: {0}".format(x_image)

        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)

        print "h_conv1: {0}".format(h_conv1)

        h_pool1 = max_pool_2x2(h_conv1)

        print "h_pool1: {0}".format(h_pool1)


        W_conv2 = weight_variable([3,30,10, 20])
        b_conv2 = bias_variable([20])

        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)

        print "h_conv2: {0}".format(h_conv2)

        h_pool2 = max_pool_2x2(h_conv2)

        print "h_pool2: {0}".format(h_pool2)

        h_conv1_flat = tf.reshape(h_pool2, [batch_size, int(len_max/4.0), 1200])

        print "h_conv1_flat: {0}".format(h_conv1_flat.get_shape())

        W_red = weight_variable([1200, 20])
        b_red = bias_variable([20])

        lstm = rnn_cell.BasicLSTMCell(lstm_size)
        number_of_layers = 1
        stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)

        W = weight_variable([stacked_lstm.state_size, label_size])
        b = bias_variable([label_size])
    
        print "Modeling cost"

        cost = 0
    
        initial_state = state = stacked_lstm.zero_state(int(len_max/4.0), tf.float32)
        print "Stacked lstm"

        y = tf.zeros([label_size])
        predictions=tf.zeros([batch_size])
        cross_entropy = 0
        states_list = []

        for j in range(batch_size):
            if (j>0):
                tf.get_variable_scope().reuse_variables()
                state = initial_state
    
            h_reduced = tf.matmul(h_conv1_flat[j,:,:],W_red)+b_red
            output, state = stacked_lstm(h_reduced, state)

        
            seq_length = tf.reduce_sum(mask[j,:,:], 1)[0]
        
            states_mean = tf.reduce_mean(state, 0) # mean pooling

            states_list.append(states_mean)
        
            states_mean_drop = tf.nn.dropout(states_mean, keep_prob)

            y = tf.matmul(tf.expand_dims(states_mean_drop,1),W, transpose_a=True)+b

            indices = [[j]]  # A list of coordinates to update.

            values = tf.argmax(y,1)  # A list of values corresponding to the respective
        # coordinate in indices.

            shape = [batch_size]  # The shape of the corresponding dense tensor, same as `c`.

            delta = tf.SparseTensor(indices, values, shape)

            dense_delta = tf.sparse_tensor_to_dense(delta, default_value=0)

            predictions = predictions + tf.to_float(dense_delta)

            cross_entropy += tf.nn.softmax_cross_entropy_with_logits(y, tf.expand_dims(y_[j,:],0))
        
        states_all = tf.pack(states_list)

        cost = cross_entropy/batch_size

        print "Y shape:{0}".format(y.get_shape())

        grad_x = [tf.gradients(y[:,cl], x) for cl in range(10)]


        train_opt = tf.train.AdamOptimizer(1e-3).minimize(cost)
        init = tf.initialize_all_variables()
        sess = tf.Session()
        sess.run(init)

        print "Number of training batches: {0}".format(num_batches)

        print "Training..."
        for i in range(num_epochs):
            print "========\nEpoch: {0}\n========".format(i)
            for k in range(0,num_batches-1):
                print "batch size: {0}".format(np.size(malware_training_set[k*batch_size: (k+1)*batch_size,:,:],0))
                sess.run(train_opt, feed_dict={x:malware_training_set[k*batch_size: (k+1)*batch_size,:,:], y_:malware_training_families[k*batch_size: (k+1)*batch_size,:], keep_prob: 0.5})
                print("{0}:TRAIN_ERR={1}".format(k,cost.eval(feed_dict={x:malware_training_set[k*batch_size: (k+1)*batch_size,:,:], y_:malware_training_families[k*batch_size: (k+1)*batch_size,:], keep_prob: 1.0}, session=sess)))

        
        print "Testing..."

        num_test_batches = int(test_set_length)/batch_size

        print "Number of batches: {0}".format(num_test_batches)

        num_attempts = np.zeros(num_families)
        num_successes=np.zeros(num_families)
        accuracy = np.zeros(num_families)

        neuron_keys = []

        neuron_states = []
    
        states_list_all = []

        labels_list = []

        gradients_all=[]

        true_labels_list = []

        for l in range(num_test_batches-1):
            test_error = cost.eval(feed_dict={x:malware_test_set[l*batch_size: (l+1)*batch_size,:], y_:malware_test_families[l*batch_size: (l+1)*batch_size,:], keep_prob: 1.0}, session=sess)
            computed_labels = predictions.eval(feed_dict={x:malware_test_set[l*batch_size: (l+1)*batch_size,:,:], y_:malware_test_families[l*batch_size: (l+1)*batch_size,:], keep_prob:1.0}, session=\
sess)
            neuron_state = h_conv1.eval(feed_dict={x:malware_test_set[l*batch_size: (l+1)*batch_size,:,:], y_:malware_test_families[l*batch_size: (l+1)*batch_size,:], keep_prob:1.0}, session=\
sess)
            true_labels = np.argmax(malware_test_families[l*batch_size: (l+1)*batch_size,:],1)

            states_list_all.append(states_all.eval(feed_dict={x:malware_test_set[l*batch_size: (l+1)*batch_size,0:100,0:dict_size], y_:malware_test_families[l*batch_size: (l+1)*batch_size,:], keep_prob:1.0}, session=sess))


            gradient_x = [ grad[0].eval(feed_dict={x:malware_test_set[l*batch_size: (l+1)*batch_size,:,:], y_:malware_test_families[l*batch_size: (l+1)*batch_size,:], keep_prob:1.0}, session=sess) for grad in grad_x]
            gradients_all.append(gradient_x)

            print "States list length:{0}".format(len(states_list_all))

            print "Computed labels\n================"
            print computed_labels
            print "True labels\n=================="
            print true_labels

            for m in range(batch_size):
                test_conf_matrix[true_labels[m], computed_labels[m]]+=1
                if (computed_labels[m]==true_labels[m]):
                       num_successes[true_labels[m]]+=1
                num_attempts[true_labels[m]]+=1

            neuron_keys.append(malware_test_set[l*batch_size: (l+1)*batch_size, 0:5, :])
            neuron_states.append(neuron_state[:,0:5,:,:])

            labels_list.append(computed_labels)
            true_labels_list.append(true_labels)

        labels_test = np.concatenate(labels_list,0)


        states_stack = np.vstack(states_list_all)
        print "States shape:{0} {1}".format(np.size(states_stack,0), np.size(states_stack,1))

        states_test = np.hstack((np.expand_dims(labels_test,1), states_stack))

        print "States test shape:{0} {1}".format(np.size(states_test,0), np.size(states_test,1))

        states_file = open('states_results.csv', 'w+')

        results = open('neuron_results.bin', 'wb+')

        pickle.dump((neuron_keys, neuron_states, word_list), results)

        np.savetxt("states_file_combined.csv", states_test, delimiter=',')

        file_out = open('gradient.bin', 'wb+')

        pickle.dump((gradients_all,labels_list, true_labels_list), file_out)

        file_out.close()


        precision_all = np.zeros(num_families)
        recall_all = np.zeros(num_families)

        for i in range(num_families):
            tp = test_conf_matrix[i,i]
            fn = 0
            fp = 0
            for j in range(num_families):
                if (j!=i):
                    fn += test_conf_matrix[i, j]
                    fp += test_conf_matrix[j,i]

            precision = tp/(tp+fp)
            recall = tp/(tp+fn)
            accuracy[i] = num_successes[i]/float(num_attempts[i])
            print "{0} ACC: {1} PR: {2} RC: {3} TP: {4} FP: {5}, FN: {6} ".format(i, accuracy[i], precision, recall, tp, fp, fn)


            precision_all[i] = np.nan_to_num(precision)
            recall_all[i] = np.nan_to_num(recall)

            precision_all_tests[i].append(precision_all[i])
            recall_all_tests[i].append(recall_all[i])

            accuracy_all_tests[i].append(accuracy[i])
    
        accuracy_cv.append(accuracy_all_tests)
        precision_cv.append(precision_all_tests)
        recall_cv.append(recall_all_tests)

    print "Confusion matrix..."
    print test_conf_matrix
    mean_precision =  [np.nanmean(precision) for precision in precision_all_tests] # for all families, get mean precision for test
    mean_recall = [np.nanmean(recall) for recall in recall_all_tests] # for all families, get mean recall averaged over tests

    print "Results averaged out"

    print np.nanmean(mean_precision) # print precision averaged over families as well
    print np.nanmean(mean_recall) # print recall averaged over families as well

    pickle.dump((accuracy_cv, precision_cv, recall_cv), open('nn_out_stats_{0}.bin'.format(test), 'wb+'))


#print "ACC: {0} PR: {1} RC: {2}".format(np.nanmean(accuracy), np.nanmean(precision_all), np.nanmean(recall_all))







