
##############################################################                                                                                                                         
# Code used for testing the combination of FNN and CNN for malware classification                                                                   
# Needed data: Malware instruction sequences, PE Header data                                                                                                                          
# Result: Model that assigns a label to previously unknown malware sample                                                                                                             
#############################################################    

import sys
import os
import time
import pickle
import random

import numpy as np
import numpy

from sklearn import preprocessing
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler


from sklearn.utils import shuffle

from collections import Counter


import tensorflow as tf

import matplotlib.pyplot as plt

def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_1(x):
    return tf.nn.avg_pool(x, ksize=[1, 2,1, 1],
                        strides=[1, 2, 1, 1], padding='SAME')

def max_pool_2(x):
    return tf.nn.avg_pool(x, ksize=[1, 250,1, 1],
                        strides=[1, 250, 1, 1], padding='SAME')

class NN:
    def __init__(self, result_path, output_units, model, layers=1, conv_filter_length=2):        

        self.result_path = result_path
        self.output_units = output_units
        self.model = model
        self.layers = layers
        self.conv_filter_length = conv_filter_length

        self.X_int = pickle.load(open(os.path.join(self.result_path, 'malware_int_features_matrix.p'), "rb"))
        self.X_int = preprocessing.MinMaxScaler().fit_transform(self.X_int)

        self.X_dll_func = pickle.load(open(os.path.join(self.result_path, 'malware_boolean_features_matrix.p'), "rb"))

        self.X_asm = pickle.load(open(os.path.join(self.result_path, 'malware_asm_features_matrix.p'), "rb"))
        self.X_asm2 = pickle.load(open(os.path.join(self.result_path, 'malware_asm_features2_matrix.p'), "rb"))

        self.X_mlp = np.concatenate((self.X_int, self.X_dll_func), axis=1)
        self.X_cnn = np.concatenate((self.X_asm, self.X_asm2), axis=1)

        self.y = np.genfromtxt(os.path.join(self.result_path,'dbscan_labels.csv'), dtype=numpy.float64)

    def one_hot_encode_opcodes(self, opcodes, excerpt):
        one_hot_encode = np.empty((0, 1500), dtype=numpy.float64)
        for i in xrange(excerpt.start, excerpt.stop):            
            x = np.zeros((1500, 118), dtype=numpy.float64)
            for j in xrange(0, 1500):
                x[j, int(opcodes[i, j])] = True
            x = np.transpose(x)
            one_hot_encode = np.append(one_hot_encode, x, axis=0)
        return one_hot_encode


    def load_dataset_train(self, batchsize):
        count_batches = 0
        for start_idx in range(0, len(self.X_train_mlp) - batchsize + 1, batchsize):
            excerpt = slice(start_idx, start_idx + batchsize)
            if self.model == 'CNN' or self.model == 'Integrated':
                one_hot_encode = self.one_hot_encode_opcodes(self.X_train_cnn, excerpt).T
            else:
                one_hot_encode = np.zeros((118, 1500), dtype=theano.config.floatX)
            count_batches+=1
            
            yield self.X_train_mlp[excerpt], one_hot_encode.reshape(-1, 1500, 118), self.y_train_binary[excerpt,:]

        print "Number of batches: {0}".format(count_batches)

    def load_dataset_validation(self, batchsize):
        for start_idx in range(0, len(self.X_val_mlp) - batchsize + 1, batchsize):
            excerpt = slice(start_idx, start_idx + batchsize)
            if self.model == 'CNN' or self.model == 'Integrated':
                one_hot_encode = self.one_hot_encode_opcodes(self.X_val_cnn, excerpt)
            else:
                one_hot_encode = np.zeros((118, 1500), dtype=theano.config.floatX)
            yield self.X_val_mlp[excerpt], one_hot_encode.reshape(-1, 1500, 118), self.y_val_binary[excerpt]

    def load_dataset_test(self, batchsize):
        for start_idx in range(0, len(self.X_test_mlp) - batchsize + 1, batchsize):
            excerpt = slice(start_idx, start_idx + batchsize)
            if self.model == 'CNN' or self.model == 'Integrated':
                one_hot_encode = self.one_hot_encode_opcodes(self.X_test_cnn, excerpt)
            else:
                one_hot_encode = np.zeros((118, 1500), dtype=theano.config.floatX)
            yield self.X_test_mlp[excerpt], one_hot_encode.reshape(-1, 1500,118), self.y_test[excerpt]

    def build_mlp(self, input_var_1=None, features_size=0):
        
        print input_var_1
        W_ff_1 = weight_variable([525,525]) # mlp                                                                                                                                
        b_ff_1 = bias_variable([525])
        W_ff_2 = weight_variable([525,525]) # 3-grams, width of the filter                                                                                                                                
        b_ff_2 = bias_variable([525])

        h = tf.nn.relu(tf.matmul(input_var_1, W_ff_1)+b_ff_1)
        h_2 = tf.nn.relu(tf.matmul(h, W_ff_2)+b_ff_2)

        return h_2

    def build_cnn(self, input_var_cnn=None):

        W_conv1 = weight_variable([3,118,1,3]) # 3-grams, width of the filter                                                                                                                                
        b_conv1 = bias_variable([3])


        x_image = tf.expand_dims(input_var_cnn,3)

        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)

        h_pool1 = max_pool_1(h_conv1)



        W_conv2 = weight_variable([3,118,3, 6])
        b_conv2 = bias_variable([6])

        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)

        h_pool2 = max_pool_2(h_conv2)

        h_conv1_flat = tf.reshape(h_pool2, [-1, 118*6])

        return h_conv1_flat

    def build_network(self):
     
        label_size=13
        dict_size_mlp=525
        dict_size_cnn=118
        len_max = 500

        print("Building model and compiling functions...")

        self.target = tf.placeholder("float32", shape=[None, label_size])
        self.keep_prob = tf.placeholder("float")

        if self.model == 'FFNN':
            self.x_in_ff = tf.placeholder("float", shape=[None, dict_size_mlp])

            self.h_inter = self.build_mlp(self.x_in_ff, features_size=525)
        elif self.model == 'CNN':
            self.x_in_cnn = tf.placeholder("float", shape=[None, len_max, dict_size_cnn])
            self.h_inter = self.build_cnn(self.x_in_cnn)
        else:
            self.x_in_ff = tf.placeholder("float", shape=[None, dict_size_mlp])
            self.x_in_cnn = tf.placeholder("float", shape=[None,  len_max, dict_size_cnn])


            network_mlp = self.build_mlp(self.x_in_ff, features_size=525)
            network_cnn = self.build_cnn(self.x_in_cnn)
            print network_mlp
            print network_cnn
            self.h_inter = tf.concat([network_mlp, network_cnn],1)

        self.h_out = tf.nn.dropout(self.h_inter, self.keep_prob)

        self.W_out = weight_variable([1233, 13])
        self.b_out = bias_variable([13])

        self.y_raw = tf.matmul(self.h_inter, self.W_out)+ self.b_out

        self.y_out = tf.nn.softmax(self.y_raw)

        self.grad_ffnn = tf.gradients(self.y_out, self.x_in_ff)
        self.grad_cnn = tf.gradients(self.y_out, self.x_in_cnn)

        self.grad_ffnn_raw = [tf.gradients(self.y_raw[:,cl], self.x_in_ff) for cl in range(13)]
        self.grad_cnn_raw = [tf.gradients(self.y_raw[:,cl], self.x_in_cnn) for cl in range(13)]

        self.loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_out, labels=self.target))

        self.labels = tf.argmax(self.y_out,1)


    def test_model(self, iteration):
        print "============================= TESTING ==============================="
        file_out = open('results_{0}.bin'.format(iteration), 'wb')
        test_batches = 0
        conf_matrix = np.zeros((13,13), dtype=np.int32)
        test_attempts = np.zeros((13,1), dtype=np.int32)
        test_success = np.zeros((13,1), dtype=np.int32)
        test_err=0
        y_computed_all = []
        y_targets_all = []
        y_gradients_ffnn_all = []
        y_gradients_cnn_all = []
        for batch_id,batch in enumerate(self.load_dataset_test(20)):
            print batch_id
            inputs_mlp, inputs_cnn, targets = batch

            num_y = len(targets)
            y_test_binary = np.zeros((num_y,13))

            for i in range(num_y):

                y_test_binary[i, targets[i]]=1 # binary encoding                                                                                           

            gradient_cnn = [grad[0].eval(feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn[:,0:500,:], self.target:y_test_binary, self.keep_prob: 1.0}, session=self.sess) for grad in self.grad_cnn_raw]                                                                         
            gradient_ffnn = [grad[0].eval(feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn[:,0:500,:], self.target:y_test_binary, self.keep_prob: 1.0}, session=self.sess) for grad in self.grad_ffnn_raw]
            
            num_samples = len(targets)
            for i in range(num_samples):
                y_gradients_cnn_all.append([gradient_cnn[cl][i,:,:] for cl in range(13)])
                y_gradients_ffnn_all.append([gradient_ffnn[cl][i,:,] for cl in range(13)])

            y_err = self.loss.eval(feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn[:,0:500,:], self.target:y_test_binary, self.keep_prob: 1.0}, session=self.sess)
            y_prediction_targets = self.labels.eval(feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn[:,0:500,:], self.target:y_test_binary, self.keep_prob: 1.0}, session=self.sess)
            test_batches+=1
            test_err+=y_err
            for i in range(num_samples):
                
                true_label = targets[i]
                test_attempts[true_label]+=1
                computed_label = y_prediction_targets[i]
                conf_matrix[true_label,computed_label]+=1
                if (true_label==computed_label):
                    test_success[true_label]+=1
                y_computed_all.append(computed_label)
                y_targets_all.append(true_label)


        print("Final results:")
        print("  test loss:\t\t\t{:.6f}".format(test_err / test_batches))
        test_accuracy = np.divide((test_success*100),test_attempts)
        print "test_accuracy"
        print test_accuracy
        print("  test confusion matrix:\t\t {0}".format(conf_matrix))
        print "Result size: {0} {1} {2}".format(len(y_computed_all), len(y_targets_all), len(y_gradients_cnn_all), len(y_gradients_cnn_all[0]))
        pickle.dump((y_computed_all, y_targets_all, y_gradients_cnn_all[0:100], y_gradients_ffnn_all[0:100]),file_out)
        file_out.close()

    def train_model(self, iteration, num_epochs=200):

        print("Starting training...")
        # We iterate over epochs:

        train_opt = tf.train.AdamOptimizer(1e-3).minimize(self.loss)
        init = tf.initialize_all_variables()
        self.sess = tf.Session()
        self.sess.run(init)
        print "=========================================== TRAINING TIME ========================================="
        for epoch in range(num_epochs):
            # In each epoch, we do a full pass over the training data:
            train_err = 0
            train_batches = 0
            start_time = time.time()
            
            for batch_id, batch in enumerate(self.load_dataset_train(20)):
                inputs_mlp, inputs_cnn, targets = batch                

                if self.model == 'FFNN': # not working
                    self.sess.run(train_opt, feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn[:,:,1:500], self.y_out:np.float64(targets), self.keep_prob: 0.7})
                    train_err+= self.loss.eval(feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn, self.y_out:targets, self.keep_prob: 1.0}, session=self.sess)
                elif self.model == 'CNN': # not working
                    self.sess.run(train_opt, feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn, self.y_out:np.float64(targets), self.keep_prob: 0.5})
                    train_err+= self.loss.eval(feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn, self.y_out:targets, self.keep_prob: 1.0}, session=self.sess)
                else:
                    self.sess.run(train_opt, feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn[:,0:500,:], self.target:targets, self.keep_prob: 0.9})
                    train_err_new= self.loss.eval(feed_dict={self.x_in_ff:inputs_mlp, self.x_in_cnn:inputs_cnn[:,0:500,:], self.target:targets, self.keep_prob: 1.0}, session=self.sess)
                    print "{0}: {1}".format(batch_id, train_err_new)
                    train_err+=train_err_new



            print "Epoch: {0}, Cost: {1}".format(epoch, train_err)


if __name__ == '__main__':

    nn_instance = NN(result_path=os.path.join(''), output_units=13, model="Integrated", layers=2, conv_filter_length=3)

    i = 0
    train_test = StratifiedShuffleSplit(nn_instance.y, n_iter=3, test_size=0.4, random_state=0)
    tf.set_random_seed(1337)
    for training_set_indices_i, testing_set_indices_i in train_test:

        nn_instance.training_set_length = len(training_set_indices_i)
        nn_instance.X_train_mlp, nn_instance.X_train_cnn, nn_instance.y_train = nn_instance.X_mlp[training_set_indices_i], nn_instance.X_cnn[training_set_indices_i], nn_instance.y[training_set_indices_i]

        nn_instance.nn_all_x = np.hstack((nn_instance.X_train_mlp, nn_instance.X_train_cnn))

        ros = RandomUnderSampler(random_state=42)
        X_res, y_res = ros.fit_sample(nn_instance.nn_all_x, nn_instance.y_train)

        X_res, y_res = shuffle(X_res, y_res, random_state=0)
        
        print "Initial shape: {0}".format(nn_instance.X_train_mlp.shape)
        print "Resulting shape: {0}".format(X_res.shape)

        print('Initial dataset shape {}'.format(Counter(nn_instance.y_train)))
        print('Resampled dataset shape {}'.format(Counter(y_res)))

        nn_instance.X_train_mlp = X_res[:, 0:525]
        nn_instance.X_train_cnn = X_res[:,525:]
        nn_instance.y_train = y_res

        num_y = np.size(nn_instance.y_train)
        y_train_binary = np.zeros((num_y,13))

        for tr_i in range(num_y):
            y_train_binary[tr_i, int(nn_instance.y_train[tr_i])]=1 # binary encoding

        nn_instance.y_train_binary = y_train_binary


        nn_instance.X_test_mlp, nn_instance.X_test_cnn, nn_instance.y_test = nn_instance.X_mlp[testing_set_indices_i],nn_instance.X_cnn[testing_set_indices_i], nn_instance.y[testing_set_indices_i] 
       

        print('iter', i)
        nn_instance.build_network()
        print('train')
        nn_instance.train_model(iteration=i)
        print('test')
        nn_instance.test_model(iteration=i)

        i += 1
